{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-17T13:15:25.093646Z",
     "iopub.status.busy": "2025-11-17T13:15:25.093429Z",
     "iopub.status.idle": "2025-11-17T13:15:26.838419Z",
     "shell.execute_reply": "2025-11-17T13:15:26.837504Z",
     "shell.execute_reply.started": "2025-11-17T13:15:25.093618Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:09:00.755281Z",
     "iopub.status.busy": "2025-11-17T13:09:00.754665Z",
     "iopub.status.idle": "2025-11-17T13:09:09.722232Z",
     "shell.execute_reply": "2025-11-17T13:09:09.721558Z",
     "shell.execute_reply.started": "2025-11-17T13:09:00.755255Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting roboflow\n",
      "  Downloading roboflow-1.2.11-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.10.5)\n",
      "Collecting idna==3.7 (from roboflow)\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.7.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.26.4)\n",
      "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.3.0)\n",
      "Collecting pi-heif<2 (from roboflow)\n",
      "  Downloading pi_heif-1.1.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pillow-avif-plugin<2 (from roboflow)\n",
      "  Downloading pillow_avif_plugin-1.5.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.2.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.5.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.3)\n",
      "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
      "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.2.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2.4.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.59.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (25.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.0.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.4)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.5->roboflow) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.18.5->roboflow) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.18.5->roboflow) (2024.2.0)\n",
      "Downloading roboflow-1.2.11-py3-none-any.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pi_heif-1.1.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp311-cp311-manylinux_2_28_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pillow-avif-plugin, pi-heif, idna, opencv-python-headless, roboflow\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.11\n",
      "    Uninstalling idna-3.11:\n",
      "      Successfully uninstalled idna-3.11\n",
      "  Attempting uninstall: opencv-python-headless\n",
      "    Found existing installation: opencv-python-headless 4.12.0.88\n",
      "    Uninstalling opencv-python-headless-4.12.0.88:\n",
      "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "s3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2025.10.0 which is incompatible.\n",
      "ray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\n",
      "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.1 pillow-avif-plugin-1.5.2 roboflow-1.2.11\n"
     ]
    }
   ],
   "source": [
    "!pip install roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:09:11.263258Z",
     "iopub.status.busy": "2025-11-17T13:09:11.263004Z",
     "iopub.status.idle": "2025-11-17T13:09:18.038768Z",
     "shell.execute_reply": "2025-11-17T13:09:18.038144Z",
     "shell.execute_reply.started": "2025-11-17T13:09:11.263234Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in defects-in-timber-5 to yolov8:: 100%|██████████| 251906/251906 [00:03<00:00, 83705.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to defects-in-timber-5 in yolov8:: 100%|██████████| 11946/11946 [00:01<00:00, 8650.70it/s] \n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"your_api_key_here\")\n",
    "project = rf.workspace(\"mushroom-species\").project(\"defects-in-timber\")\n",
    "version = project.version(5)\n",
    "dataset = version.download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T12:39:07.763820Z",
     "iopub.status.busy": "2025-11-17T12:39:07.763078Z",
     "iopub.status.idle": "2025-11-17T12:39:08.099574Z",
     "shell.execute_reply": "2025-11-17T12:39:08.098933Z",
     "shell.execute_reply.started": "2025-11-17T12:39:07.763792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "if os.path.exists(\"defects-in-timber-5\"):\n",
    "    shutil.rmtree(\"defects-in-timber-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:15:57.220912Z",
     "iopub.status.busy": "2025-11-17T13:15:57.220585Z",
     "iopub.status.idle": "2025-11-17T13:15:57.225534Z",
     "shell.execute_reply": "2025-11-17T13:15:57.224737Z",
     "shell.execute_reply.started": "2025-11-17T13:15:57.220884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = \"kaggle_username_here\"\n",
    "os.environ['KAGGLE_KEY'] = \"kaggle_key_here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:15:59.360767Z",
     "iopub.status.busy": "2025-11-17T13:15:59.360181Z",
     "iopub.status.idle": "2025-11-17T13:16:03.741327Z",
     "shell.execute_reply": "2025-11-17T13:16:03.740345Z",
     "shell.execute_reply.started": "2025-11-17T13:15:59.360741Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/umarfarooq211203/defects-in-timber\n",
      "License(s): CC0-1.0\n",
      "Downloading defects-in-timber.zip to /kaggle/working\n",
      " 77%|██████████████████████████████▊         | 338M/438M [00:00<00:00, 1.18GB/s]\n",
      "100%|████████████████████████████████████████| 438M/438M [00:00<00:00, 1.18GB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download umarfarooq211203/defects-in-timber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:18:16.108472Z",
     "iopub.status.busy": "2025-11-17T13:18:16.108176Z",
     "iopub.status.idle": "2025-11-17T13:18:20.788322Z",
     "shell.execute_reply": "2025-11-17T13:18:20.787616Z",
     "shell.execute_reply.started": "2025-11-17T13:18:16.108453Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files extracted to /kaggle/working/defects-in-timber-5\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Path to your zip file\n",
    "zip_path = '/kaggle/working/defects-in-timber.zip'\n",
    "\n",
    "# Directory where you want to extract the files\n",
    "extract_dir = '/kaggle/working/defects-in-timber-5'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Open the zip file and extract all contents\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(f\"Files extracted to {extract_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:18:53.327795Z",
     "iopub.status.busy": "2025-11-17T13:18:53.327170Z",
     "iopub.status.idle": "2025-11-17T13:18:53.810269Z",
     "shell.execute_reply": "2025-11-17T13:18:53.809531Z",
     "shell.execute_reply.started": "2025-11-17T13:18:53.327770Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== TRAIN ====\n",
      "Images per class: {0: 2628, 1: 2772, 2: 3431, 3: 2866}\n",
      "Objects per class: {0: 3246, 1: 3897, 2: 5431, 3: 3445}\n",
      "\n",
      "==== VALID ====\n",
      "Images per class: {0: 365, 1: 376, 2: 488, 3: 406}\n",
      "Objects per class: {0: 454, 1: 510, 2: 778, 3: 494}\n",
      "\n",
      "==== TEST ====\n",
      "Images per class: {0: 254, 1: 263, 2: 305, 3: 269}\n",
      "Objects per class: {0: 307, 1: 357, 2: 506, 3: 332}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATASET_PATH = \"/kaggle/working/defects-in-timber-5\"\n",
    "splits = [\"train\", \"valid\", \"test\"]\n",
    "\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "def count_images_per_class(split):\n",
    "    label_dir = os.path.join(DATASET_PATH, split, \"labels\")\n",
    "\n",
    "    img_count = {i: 0 for i in range(NUM_CLASSES)}\n",
    "\n",
    "    for lbl in os.listdir(label_dir):\n",
    "        if not lbl.endswith(\".txt\"):\n",
    "            continue\n",
    "        \n",
    "        classes_in_file = set()\n",
    "        with open(os.path.join(label_dir, lbl)) as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                try:\n",
    "                    cls = int(float(parts[0]))    # SAFE PARSE\n",
    "                    classes_in_file.add(cls)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        for cls in classes_in_file:\n",
    "            img_count[cls] += 1\n",
    "\n",
    "    return img_count\n",
    "\n",
    "\n",
    "def count_objects_per_class(split):\n",
    "    label_dir = os.path.join(DATASET_PATH, split, \"labels\")\n",
    "\n",
    "    obj_count = {i: 0 for i in range(NUM_CLASSES)}\n",
    "\n",
    "    for lbl in os.listdir(label_dir):\n",
    "        if not lbl.endswith(\".txt\"):\n",
    "            continue\n",
    "        with open(os.path.join(label_dir, lbl)) as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                try:\n",
    "                    cls = int(float(parts[0]))\n",
    "                    obj_count[cls] += 1\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    return obj_count\n",
    "\n",
    "\n",
    "# ---- PRINT COUNTS ----\n",
    "for split in splits:\n",
    "    print(f\"\\n==== {split.upper()} ====\")\n",
    "    print(\"Images per class:\", count_images_per_class(split))\n",
    "    print(\"Objects per class:\", count_objects_per_class(split))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:09:30.882693Z",
     "iopub.status.busy": "2025-11-17T13:09:30.882025Z",
     "iopub.status.idle": "2025-11-17T13:09:35.946483Z",
     "shell.execute_reply": "2025-11-17T13:09:35.945755Z",
     "shell.execute_reply.started": "2025-11-17T13:09:30.882669Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, Rotate, ShiftScaleRotate, Compose\n",
    ")\n",
    "\n",
    "DATASET_PATH = \"/kaggle/working/defects-in-timber-5\"\n",
    "splits = [\"train\",\"test\",\"valid\"]\n",
    "\n",
    "TARGET_COUNT = {\"train\":3200,\"test\":300,\"valid\":450}\n",
    "CLASSES_TO_AUGMENT = [0, 3]   \n",
    "#CLASSES_TO_AUGMENT = [1]   \n",
    "# Crack, knot_with_crack\n",
    "\n",
    "transform = Compose([\n",
    "    HorizontalFlip(p=0.5),\n",
    "    VerticalFlip(p=0.5),\n",
    "    Rotate(limit=20, p=0.5),\n",
    "    ShiftScaleRotate(\n",
    "        shift_limit=0.05,\n",
    "        scale_limit=0.10,\n",
    "        rotate_limit=0,\n",
    "        border_mode=cv2.BORDER_CONSTANT,\n",
    "        p=0.5\n",
    "    )\n",
    "], bbox_params={'format': 'yolo', 'label_fields': ['class_labels']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:09:35.948452Z",
     "iopub.status.busy": "2025-11-17T13:09:35.947881Z",
     "iopub.status.idle": "2025-11-17T13:09:35.953809Z",
     "shell.execute_reply": "2025-11-17T13:09:35.953059Z",
     "shell.execute_reply.started": "2025-11-17T13:09:35.948432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_annotations(label_file):\n",
    "    bboxes, class_labels = [], []\n",
    "\n",
    "    with open(label_file) as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 5:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                cls = int(float(parts[0]))\n",
    "                x, y, w, h = map(float, parts[1:])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            class_labels.append(cls)\n",
    "            bboxes.append([x, y, w, h])  # already normalized YOLO\n",
    "    return bboxes, class_labels\n",
    "\n",
    "\n",
    "def save_annotations(label_file, bboxes, class_labels):\n",
    "    with open(label_file, \"w\") as f:\n",
    "        for cls, (x, y, w, h) in zip(class_labels, bboxes):\n",
    "            f.write(f\"{cls} {x:.6f} {y:.6f} {w:.6f} {h:.6f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:09:35.955100Z",
     "iopub.status.busy": "2025-11-17T13:09:35.954756Z",
     "iopub.status.idle": "2025-11-17T13:09:54.863972Z",
     "shell.execute_reply": "2025-11-17T13:09:54.863368Z",
     "shell.execute_reply.started": "2025-11-17T13:09:35.955073Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AUGMENTING: TRAIN ===\n",
      "Class 0: Need 1704 additional objects.\n",
      "✓ Added approximately 1705 objects for class 0\n",
      "\n",
      "Class 3: Need 554 additional objects.\n",
      "✓ Added approximately 554 objects for class 3\n",
      "\n",
      "\n",
      "=== AUGMENTING: TEST ===\n",
      "Class 0: Need 167 additional objects.\n",
      "✓ Added approximately 167 objects for class 0\n",
      "\n",
      "Class 3: Need 80 additional objects.\n",
      "✓ Added approximately 80 objects for class 3\n",
      "\n",
      "\n",
      "=== AUGMENTING: VALID ===\n",
      "Class 0: Need 254 additional objects.\n",
      "✓ Added approximately 254 objects for class 0\n",
      "\n",
      "Class 3: Need 38 additional objects.\n",
      "✓ Added approximately 38 objects for class 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_objects(split):\n",
    "    return count_objects_per_class(split)\n",
    "\n",
    "for split in splits:\n",
    "    print(f\"\\n=== AUGMENTING: {split.upper()} ===\")\n",
    "\n",
    "    label_dir = os.path.join(DATASET_PATH, split, \"labels\")\n",
    "    image_dir = os.path.join(DATASET_PATH, split, \"images\")\n",
    "\n",
    "    counts = count_objects(split)\n",
    "\n",
    "    for cls in CLASSES_TO_AUGMENT:\n",
    "        need = TARGET_COUNT[split] - counts[cls]\n",
    "        if need <= 0:\n",
    "            print(f\"Class {cls} already >= target.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Class {cls}: Need {need} additional objects.\")\n",
    "\n",
    "        # Get all files containing this class\n",
    "        label_files = []\n",
    "        for f in os.listdir(label_dir):\n",
    "            if f.endswith(\".txt\"):\n",
    "                txt_path = os.path.join(label_dir, f)\n",
    "                content = open(txt_path).read().split()\n",
    "                if str(cls) in content:\n",
    "                    label_files.append(f)\n",
    "\n",
    "        if len(label_files) == 0:\n",
    "            print(f\"No images containing class {cls}.\")\n",
    "            continue\n",
    "\n",
    "        added = 0\n",
    "        idx = 0\n",
    "\n",
    "        while added < need:\n",
    "            lbl = label_files[idx % len(label_files)]\n",
    "            base = lbl.replace(\".txt\", \"\")\n",
    "\n",
    "            # find image\n",
    "            img_path = None\n",
    "            for ext in (\".jpg\", \".png\", \".jpeg\"):\n",
    "                fp = os.path.join(image_dir, base + ext)\n",
    "                if os.path.exists(fp):\n",
    "                    img_path = fp\n",
    "                    extension = ext\n",
    "                    break\n",
    "            if img_path is None:\n",
    "                idx += 1\n",
    "                continue\n",
    "\n",
    "            image = cv2.imread(img_path)\n",
    "\n",
    "            bboxes, class_labels = load_annotations(os.path.join(label_dir, lbl))\n",
    "            if len(bboxes) == 0:\n",
    "                idx += 1\n",
    "                continue\n",
    "\n",
    "            augmented = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "\n",
    "            if len(augmented[\"bboxes\"]) == 0:\n",
    "                idx += 1\n",
    "                continue\n",
    "\n",
    "            new_img = augmented[\"image\"]\n",
    "            new_bboxes = augmented[\"bboxes\"]\n",
    "            new_labels = augmented[\"class_labels\"]\n",
    "\n",
    "            new_name = f\"{base}_aug_{added}\"\n",
    "            cv2.imwrite(os.path.join(image_dir, new_name + extension), new_img)\n",
    "            save_annotations(os.path.join(label_dir, new_name + \".txt\"), new_bboxes, new_labels)\n",
    "\n",
    "            added += new_labels.count(cls)\n",
    "            idx += 1\n",
    "\n",
    "        print(f\"✓ Added approximately {added} objects for class {cls}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:09:58.764766Z",
     "iopub.status.busy": "2025-11-17T13:09:58.764010Z",
     "iopub.status.idle": "2025-11-17T13:09:59.183834Z",
     "shell.execute_reply": "2025-11-17T13:09:59.183016Z",
     "shell.execute_reply.started": "2025-11-17T13:09:58.764737Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== TRAIN ====\n",
      "Images per class: {0: 2628, 1: 2772, 2: 3431, 3: 2866}\n",
      "Objects per class: {0: 3246, 1: 3897, 2: 5431, 3: 3445}\n",
      "\n",
      "==== TEST ====\n",
      "Images per class: {0: 254, 1: 263, 2: 305, 3: 269}\n",
      "Objects per class: {0: 307, 1: 357, 2: 506, 3: 332}\n",
      "\n",
      "==== VALID ====\n",
      "Images per class: {0: 365, 1: 376, 2: 488, 3: 406}\n",
      "Objects per class: {0: 454, 1: 510, 2: 778, 3: 494}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- PRINT COUNTS ----\n",
    "for split in splits:\n",
    "    print(f\"\\n==== {split.upper()} ====\")\n",
    "    print(\"Images per class:\", count_images_per_class(split))\n",
    "    print(\"Objects per class:\", count_objects_per_class(split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Dataset in Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:10:19.765916Z",
     "iopub.status.busy": "2025-11-17T13:10:19.765291Z",
     "iopub.status.idle": "2025-11-17T13:10:35.817970Z",
     "shell.execute_reply": "2025-11-17T13:10:35.817210Z",
     "shell.execute_reply.started": "2025-11-17T13:10:19.765893Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/defects-in-timber.zip'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "dataset_folder = \"/kaggle/working/defects-in-timber-5\"\n",
    "output_zip = \"/kaggle/working/defects-in-timber.zip\"\n",
    "\n",
    "shutil.make_archive(base_name=output_zip.replace(\".zip\",\"\"),\n",
    "                    format='zip',\n",
    "                    root_dir=dataset_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:10:38.909464Z",
     "iopub.status.busy": "2025-11-17T13:10:38.908996Z",
     "iopub.status.idle": "2025-11-17T13:10:38.914314Z",
     "shell.execute_reply": "2025-11-17T13:10:38.913580Z",
     "shell.execute_reply.started": "2025-11-17T13:10:38.909440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "        # Example of creating a metadata file\n",
    "dataset_dir = \"/kaggle/dataset/defects-in-timber\"\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "import json\n",
    "metadata = {\n",
    "    \"title\": \"Defects-in-Timber Dataset\",\n",
    "    \"id\": f\"{os.environ['KAGGLE_USERNAME']}/defects-in-timber\",\n",
    "    \"licenses\": [{\"name\": \"CC0-1.0\"}]  # <-- This is required\n",
    "}\n",
    "with open(os.path.join(dataset_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "    json.dump(metadata, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:10:40.127060Z",
     "iopub.status.busy": "2025-11-17T13:10:40.126792Z",
     "iopub.status.idle": "2025-11-17T13:10:42.401688Z",
     "shell.execute_reply": "2025-11-17T13:10:42.400898Z",
     "shell.execute_reply.started": "2025-11-17T13:10:40.127039Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset files copied!\n"
     ]
    }
   ],
   "source": [
    "src_folder = \"/kaggle/working/defects-in-timber-5\"\n",
    "dst_folder = \"/kaggle/dataset/defects-in-timber\"\n",
    "\n",
    "# Copy all files and subfolders\n",
    "for item in os.listdir(src_folder):\n",
    "    s = os.path.join(src_folder, item)\n",
    "    d = os.path.join(dst_folder, item)\n",
    "    if os.path.isdir(s):\n",
    "        shutil.copytree(s, d, dirs_exist_ok=True)\n",
    "    else:\n",
    "        shutil.copy2(s, d)\n",
    "\n",
    "print(\"Dataset files copied!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:10:44.814492Z",
     "iopub.status.busy": "2025-11-17T13:10:44.813959Z",
     "iopub.status.idle": "2025-11-17T13:11:10.777259Z",
     "shell.execute_reply": "2025-11-17T13:11:10.776317Z",
     "shell.execute_reply.started": "2025-11-17T13:10:44.814466Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file valid.zip\n",
      "100%|██████████████████████████████████████| 48.2M/48.2M [00:00<00:00, 60.5MB/s]\n",
      "Upload successful: valid.zip (48MB)\n",
      "Starting upload for file README.dataset.txt\n",
      "100%|████████████████████████████████████████████| 155/155 [00:00<00:00, 360B/s]\n",
      "Upload successful: README.dataset.txt (155B)\n",
      "Starting upload for file test.zip\n",
      "100%|██████████████████████████████████████| 34.8M/34.8M [00:00<00:00, 52.7MB/s]\n",
      "Upload successful: test.zip (35MB)\n",
      "Starting upload for file README.roboflow.txt\n",
      "100%|██████████████████████████████████████| 1.17k/1.17k [00:00<00:00, 2.79kB/s]\n",
      "Upload successful: README.roboflow.txt (1KB)\n",
      "Starting upload for file data.yaml\n",
      "100%|████████████████████████████████████████████| 313/313 [00:00<00:00, 689B/s]\n",
      "Upload successful: data.yaml (313B)\n",
      "Starting upload for file train.zip\n",
      "100%|█████████████████████████████████████████| 354M/354M [00:02<00:00, 128MB/s]\n",
      "Upload successful: train.zip (354MB)\n",
      "Your private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/umarfarooq211203/defects-in-timber\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets create -p \"{dataset_dir}\" --dir-mode zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture Tiny Vit YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:21:51.303000Z",
     "iopub.status.busy": "2025-11-17T13:21:51.302139Z",
     "iopub.status.idle": "2025-11-17T13:21:54.942092Z",
     "shell.execute_reply": "2025-11-17T13:21:54.941208Z",
     "shell.execute_reply.started": "2025-11-17T13:21:51.302971Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics timm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiny Vit Backbone Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:21:58.002348Z",
     "iopub.status.busy": "2025-11-17T13:21:58.001571Z",
     "iopub.status.idle": "2025-11-17T13:22:06.041931Z",
     "shell.execute_reply": "2025-11-17T13:22:06.041162Z",
     "shell.execute_reply.started": "2025-11-17T13:21:58.002311Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics import YOLO\n",
    "import timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:22:16.786187Z",
     "iopub.status.busy": "2025-11-17T13:22:16.785846Z",
     "iopub.status.idle": "2025-11-17T13:22:16.792644Z",
     "shell.execute_reply": "2025-11-17T13:22:16.791797Z",
     "shell.execute_reply.started": "2025-11-17T13:22:16.786159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class TinyViTBackboneYOLO(nn.Module):\n",
    "    def __init__(self, model_name='tiny_vit_11m_224.in1k', pretrained=True):\n",
    "        super().__init__()\n",
    "        # Load TinyViT with multi-scale outputs\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, features_only=True)\n",
    "        self.out_indices = (-3, -2, -1)  # last 3 feature maps\n",
    "\n",
    "        # Adapter convs to match YOLOv8n head channels\n",
    "        feature_info = self.model.feature_info\n",
    "        self.adapters = nn.ModuleList([\n",
    "            nn.Conv2d(feature_info[i][\"num_chs\"], ch, 1)\n",
    "            for i, ch in enumerate([256, 512, 1024])\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.model(x)\n",
    "        out = [self.adapters[i](features[idx]) for i, idx in enumerate(self.out_indices)]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:22:18.480049Z",
     "iopub.status.busy": "2025-11-17T13:22:18.479520Z",
     "iopub.status.idle": "2025-11-17T13:22:20.357090Z",
     "shell.execute_reply": "2025-11-17T13:22:20.356426Z",
     "shell.execute_reply.started": "2025-11-17T13:22:18.480027Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 75.5MB/s 0.1s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c44e58ff8a447099bec549942f6301f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/44.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load pretrained YOLOv8n\n",
    "yolo_model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Replace backbone\n",
    "backbone = TinyViTBackboneYOLO(pretrained=True)\n",
    "yolo_model.model.backbone = backbone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:22:24.426066Z",
     "iopub.status.busy": "2025-11-17T13:22:24.425401Z",
     "iopub.status.idle": "2025-11-17T13:22:27.997814Z",
     "shell.execute_reply": "2025-11-17T13:22:27.996611Z",
     "shell.execute_reply.started": "2025-11-17T13:22:24.426044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torchinfo -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-17T13:23:07.054Z",
     "iopub.execute_input": "2025-11-17T13:23:02.459300Z",
     "iopub.status.busy": "2025-11-17T13:23:02.458678Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Assume input image size 640x640 with 3 channels\n",
    "summary(yolo_model.model, input_size=(1, 3, 640, 640))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-17T13:13:41.007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "yolo2 = YOLO(\"yolov8n.pt\")\n",
    "summary(yolo2.model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "yolo_model.train(\n",
    "    data=\"defects-in-timber-5/data.yaml\",\n",
    "    imgsz=640,\n",
    "    batch=8,\n",
    "    epochs=100,\n",
    "    patience=5,\n",
    "    optimizer=\"AdamW\",\n",
    "    device=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "folder_path = \"/kaggle/working/runs\"  \n",
    "archive_name = \"/kaggle/working/yolov8_tinyvit_results\"  \n",
    "shutil.make_archive(archive_name, 'zip', folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.remove(archive_name+\".zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "metrics = yolo_model.val(data=\"defects-in-timber-5/data.yaml\", batch=2, imgsz=640)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate on test dataset\n",
    "test_metrics = yolo_model.val(data=\"defects-in-timber-5/data.yaml\", split=\"test\", batch=2, imgsz=640)\n",
    "print(test_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "yolo_model = YOLO(\"/kaggle/working/runs/detect/train2/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "yolo_model.train(\n",
    "    data=\"defects-in-timber-5/data.yaml\",\n",
    "    imgsz=640,\n",
    "    batch=8,\n",
    "    epochs=20,\n",
    "    patience=5,\n",
    "    optimizer=\"AdamW\",\n",
    "    device=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "yolo_model = YOLO(\"/kaggle/working/runs/detect/train7/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate on test dataset\n",
    "test_metrics = yolo_model.val(data=\"defects-in-timber-5/data.yaml\", split=\"test\", batch=8, imgsz=640)\n",
    "print(test_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "metrics = yolo_model.val(data=\"defects-in-timber-5/data.yaml\", batch=8, imgsz=640)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "yolo_model = YOLO(\"/kaggle/working/runs/detect/train7/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "yolo_model.train(\n",
    "    data=\"defects-in-timber-5/data.yaml\",\n",
    "    imgsz=640,\n",
    "    batch=8,\n",
    "    epochs=60,\n",
    "    patience=5,\n",
    "    optimizer=\"AdamW\",\n",
    "    device=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "yolo_model = YOLO(\"/kaggle/working/runs/detect/train10/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "metrics = yolo_model.val(data=\"defects-in-timber-5/data.yaml\", batch=2, imgsz=640)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate on test dataset\n",
    "test_metrics = yolo_model.val(data=\"defects-in-timber-5/data.yaml\", split=\"test\", batch=2, imgsz=640)\n",
    "print(test_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
